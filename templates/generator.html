<!DOCTYPE html>
<html lang="en">

<!-- HEAD -->
<head>
    {% include "head.html" %}
    <title>EPIC Face Generator</title>
</head>

<!-- BODY -->
<body>
    {% include "navbar.html" %}

    <div class="container-fluid align-items-center" id="modelContainer">
        <div class="carousel-item active">
        <img id="imgMain" src="../static/images/generator_carousel.jpg" alt="Face Generator main image">
        <div class="carousel-caption d-none d-md-block">
            <h1 id="carouselHeader">Random Face Generator</h1>
            <h4 id="carouselSubhead">DCGAN trained to randomly generate a face</h4>
        </div>
        </div>
        <section id="about">
            <h1>ABOUT</h1>
            <hr id="underline">
            <h3 id ="subhead">Methods</h3>
            <p>The purpose of this section of the project was to use machine learning algorithms to generate human faces that viewers would be unable to distinguish from actual photographs of faces.</p>
            <p>To achieve our purpose, we implemented a Deep Convolutional Generative Adversarial Network (DCGAN). This methodology uses two machine learning models, a Discriminator Network and a Generator Network, which are interconnected in a feedback loop. The discriminator is a classical Convolutional Neural Network (CNN) that classifies images into categories; in this case the categories are “real” and “fake”. The discriminator is trained by presenting it with fakes created by the generator and real images from the dataset. The generator uses feedback from the discriminator to improve its ability to generate fake images that the discriminator will improperly predict are real.</p>
            <p>In narrative terms, the generator is trying to improve its ability to create fake images that will fool the discriminator, and the discriminator is trying to improve its ability to detect the generators fakes. The relationship is illustrated in the diagram below.</p>
            <figure class="figure">
                <img id="imgContent" src="../static/images/DCGANDiagramFaces.jpg" alt="Dataset"> 
                <figcaption class="figure-caption text-right">
                    <p>original image from Medium<a href="#references"><sup>[1]</sup></a><br>edited by Troy Bailey to show faces in dataset &nbsp; &copy; 2019 TEAM EPIC</p>
                </figcaption>
            </figure>
            <h3 id ="subhead">Dataset</h3>
            <p>We used the CelebA dataset made available through Kaggle.com<a href="#references"><sup>[2]</sup></a>. The dataset consists of 202,599 images of 10,177 celebrities. The dataset is annotated with 5 landmark locations (each eye, nose, and corners of the mouth) and 40 binary attributes such as male, young, beard, glasses, blonde, etc.</p>
            <figure class="figure">
                <img id="imgContent" src="../static/images/celebA.png" alt="CelebA"> 
                <figcaption class="figure-caption text-right">
                    <p>photo from CelebA dataset<a href="#references"><sup>[2]</sup></a></p>
                </figcaption>
            </figure>
        </section>

        <hr>

        <section id="process">
            <h1>PROCESS</h1>
            <hr id="underline">
            <h3 id ="subhead">DCGAN Implementation in TensorFlow</h3>
            <p>We implemented our DCGAN in TensorFlow using a model presented by Taehoon Kim<a href="#references"><sup>[3]</sup></a>.</p>
            <p>The discriminator network, illustrated in the diagram below, consists of four convolutional layers. The inputs are the pixels of the images. Each layer of the network includes a convolution, a batch normalization and a leaky ReLu to speed training. We flatten the output of the last layer and use the sigmoid activation function to get a classification. We now have a prediction P(x) whether the image is real or a fake.</p>
            <figure class="figure">
                <img id="imgContent" src="../static/images/discriminatorDiagram.png" alt="Discriminator diagram">
                <figcaption class="figure-caption text-right">
                    <p>photo from O'Reilly&copy;.com<a href="#references"><sup>[4]</sup></a></p>
                </figcaption>
            </figure>
            <p>The generator is structured essentially as the reverse of the discriminator and is illustrated in the diagram below. The generator uses four deconvolutional layers. They are the exact opposite of a convolutional layers; instead of performing convolutions until the image is transformed into simple numerical data, such as a classification, we perform deconvolutions to transform numerical data into an image. The generator takes a input, called the Z vector, puts it through a series of 3 layers each with a deconvolution, batch normalization and leaky ReLu to create an image.</p>
            <figure class="figure">
                <img id="imgContent" src="../static/images/generatorDiagram.png" alt="Generator diagram">
                <figcaption class="figure-caption text-right">
                    <p>photo from O'Reilly&copy;.com<a href="#references"><sup>[4]</sup></a></p>
                </figcaption>
            </figure>
            <p>Optimization is achieved by calculating losses for both the discriminator (when dealing with fake and real images) and the generator (when creating fake images), and using the Adam optimizer to minimize the loss.</p>
            
            <h3 id ="subhead">Network Architecture</h3>
            <!-- @TODO Ask Troy for i7 gen# - Makes a huge difference in perf capabilities -->
            <p>The training for the model was performed on a MacBook Pro laptop with a 2.8 GHz Intel Core i7 processor and 16 GB memory. Each training epoch with the full dataset required approximately 8 hours to run.</p>
        </section>
        
        <hr>

        <section id="results">
            <h1>RESULTS</h1>
            <hr id="underline">
            <p>For our purpose of generating realistic faces, numerical respresentations of losses in training don't convey success as well as simply viewing the generated images. After one epoch of training the DCGAN model on more than 200,000 images, the generator produced the sample images below. The images are clearly an <i>attempt</i> by the model to create faces, but only a few come close to being passable and none are entirely convincing.</p>
            <figure class="figure">
                <img id="imgContent" src="../static/images/z100FirstEpoch.png" alt="Results (From Gil Levi and Tal Hassner’s paper)">
                <figcaption class="figure-caption text-right">
                    <p id="generatorCaption">generated by Troy Bailey &nbsp; &copy; 2019 TEAM EPIC</p>
                </figcaption>
            </figure>
            <p>We ran 6 more training epochs on the same model&mdash;over 40 hours&mdash;and achieved the results below. The results are greatly improved, with several images being near passable fakes. However, many are still unacceptable. Upon examining the images, one can see that the model creating more convincing images. The generator is getting better at reproducing some of the variety in the original dataset&mdash;angled poses, glasses, different racial characteristics, etc.&mdash;despite having never seen any of these images.</p>
            <figure class="figure">
                    <img id="imgContent" src="../static/images/z100SeventhEpoch.png" alt="Results (From Gil Levi and Tal Hassner’s paper)">
                <figcaption class="figure-caption text-right">
                    <p id="generatorCaption">generated by Troy Bailey &nbsp; &copy; 2019 TEAM EPIC</p>
                </figcaption>
            </figure>
            <!-- @TODO Fix second sentence to be more clear - first describe discriminator then in another sentence describe the generator as it relates. -->
            <p>Instead of pursuing more training epochs, we decided to adjust parameters of the model to achieve different results. The discriminator and generator effectively encode and decode the images into a 10x10 array of floating-point numbers from -1 to 1 (the Z-vector). This encoding requires the model to simplify the variability into 100 numbers. We speculated that giving the model a larger encoding space might allow it to encode more information. We tried 12x12 and 15x15 Z-vectors with the results from a single training epoch shown below.</p>
            <div class="row">
                <div class="col-md-4">
                    <div class="thumbnail">
                        <img src="../static/images/z100FirstEpoch.png" alt="z=10x10 with one epoch" style="width:100%">
                        <div class="caption text-left">
                            <p id="generatorCaption">z = 10x10</p>
                        </div>
                    </div>
                </div>
                <div class="col-md-4">
                    <div class="thumbnail">
                        <img src="../static/images/z144FirstEpoch.png" alt="z=12x12 with one epoch" style="width:100%">
                        <div class="caption text-left">
                            <p id="generatorCaption">z = 12x12</p>
                        </div>
                    </div>
                </div>
                <div class="col-md-4">
                    <div class="thumbnail">
                        <img src="../static/images/z225FirstEpoch.png" alt="z=15x15 with one epoch" style="width:100%">
                        <div class="caption text-left">
                            <p id="generatorCaption">z = 15x15<br>generated by Troy Bailey &nbsp; &copy; 2019 TEAM EPIC</p>
                        </div>
                    </div>
                </div>
            </div>
            <p>Both the 12x12 and 15x15 Z-vector models produced better images on their first training epoch than the 10x10 Z-vector model. However, once again rather than optimize for passable images (our goal) the models were using the additional encoding space to better represent the variety of the dataset: poses, image color palette, glasses, hair color, sexual and racial characteristics.</p>
            <p>To counteract the observed effect of image variety conflicting with generating passable images, we decided to attempt to reduce the variety of the data set by using the annotated attributes. By filtering for the characteristics of female, young, no glasses, and brunette, we eliminated much of the variation while maintaining a large sample size.</p>
            <p>We further filtered for “attractive”, because it resulted in more standard poses while eliminating very unusual poses and images with helmets and goggles that would make training more difficult. Our reduced dataset was approximately 20,000 images. As such, training time was significantly reduced to a mere 30 minutes per epoch. The resulting generated faces after 10 epochs are shown below.</p>
            <figure class="figure">
                
                    <img id="imgContent" src="../static/images/selectTenthEpoch.png" alt="Generator diagram">
                </a>
                <figcaption class="figure-caption text-right">
                    <p id="generatorCaption">generated by Troy Bailey &nbsp; &copy; 2019 TEAM EPIC</p>
                </figcaption>
            </figure>
            <p>The images are more uniform as expected and supports the hypothesis that reduced variety in the training dataset would allow the DCGAN to produce more convincing images. There are still clearly fake images being produced, but the frequency of passable images has greatly increased.</p>
            <h3 id ="subhead">DCGAN in action</h3>
            <p>To provide a visual for the evolution of generated images created, we made a timelapse with training batches from the beginning of the process to the end.</p>
            <center>
                <div class="container" id="timelapseGen">
                    <iframe id="timelapseGen" src="https://player.vimeo.com/video/310882940" width="640" height="640" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
                    <p class="text-center">created by Yuta Yamaguchi &nbsp; &copy; 2019 TEAM EPIC</p>
                </div>
            </center>
        </section>

        <hr>
        
        <section id="testing">
            <h1>TESTING</h1>
            <hr id="underline">
            <p>Did we achieve our goal of using machine learning to generate images of faces that are indistinquishable to viewers from actual photos of faces?</p>
            <p>Take our “Turing Test” to decide for yourself!</p>
            <a href="https://www.pollev.com/epicfaces012" id="generatorButton" role="button" class="btn btn-primary btn-lg btn-block" aria-disabled="true" target="_blank">Click here to participate!</a>
        </section>
        
        <hr>
        
        <section id="learnings">
            <h1>LEARNINGS</h1>
            <hr id="underline">
            <p>As noted in the Keypoints portion of our project, Datasets merit careful consideration. The CelebA dataset has many uses and the various images are helpful for many applications. If for example the goal was to train a model to recognize certain individuals when presented in different poses and costumes, CelebA would be ideal. In the case of using a DCGAN to produce convincing images, the variety included in CelebA will require much more complicated model and training time to master all the variety in the dataset. For our purposes, a subset of CelebA with less variation, yielded better results.</p>
            <p>An interesting observation from this portion of the project was that it provided a good illustration of machine learning in terms of mimicking how humans learn. The process of our DCGAN learning to create faces has many parallels with humans mastering the same skill. At first children scribble and declare it’s a face. As they mature they understand that faces are round and have eyes and a mouth and therefore start drawing ‘happy faces’. They continue to progress and start adding noses, hair, shading, and color in their attempts to draw more representational faces. The following video produced from the images that the generator produced in the first training epoch illustrate this learning parallel.</p>
            <p>As the faces progress and gain features, some inbetween images disgress from the uncanny valley, to outright terrifying. A face can look scary when there is too much shading around the eyes, a large amount of white around the pupils of the eyes, facial skin having a red fleshy color, or a smile that doesn't match with the rest of the face.</p>
            <!-- Button trigger modal -->
            <button id="generatorButton" type="button" class="btn btn-primary btn-lg btn-block" data-toggle="modal" data-target="#exampleModalCenter">
                View at your own risk!
            </button>
            <!-- Modal -->
            <div class="modal fade bd-example-modal-lg" id="exampleModalCenter" tabindex="-1" role="dialog" aria-labelledby="exampleModalCenterTitle" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                    <div class="modal-content">
                        <div class="modal-header">
                            <h4 class="modal-title" id="exampleModalLongTitle"><strong>Generated Monsters</strong></h4>
                            <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                            <span aria-hidden="true">&times;</span>
                            </button>
                        </div>
                        <div class="modal-body">
                            <img id="imgContent" src="../static/images/scarygrid.png" alt="Generated Monsters">
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <hr>

        <section id="references">
            <h4>References</h4>
            <hr id="underline">
            <ol>
                <li id="refs">
                    Thalles Silva, <a href="https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394"><i>An intuitive introduction to Generative Adversarial Networks (GANs)</i></a>, Medium, Jan. 2018 
                </li>
                <li id="refs">
                    Ping Luo, Ziwei Liu, et al., <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"><i>Large-scale CelebFaces Attributes (CelebA) Dataset</i></a>, <a href="https://www.kaggle.com/jessicali9530/celeba-dataset/home">Kaggle Datasets</a>, 2018
                </li>
                <li id="refs">
                    Taehoon Kim, <a href="https://github.com/carpedm20/DCGAN-tensorflow"><i>Age and Gender Estimation of Unfiltered Faces</i></a>, GitHub, Dec. 2015
                </li>
                <li id="refs">
                    Dominic Monn, <a href="https://www.oreilly.com/ideas/deep-convolutional-generative-adversarial-networks-with-tensorflow"><i>Deep convolutional generative adversarial networks with TensorFlow</i></a>, O'Reilly&copy;, Nov 2017
                </li>
            </ol>
            <br>
            <hr id="underline">
            <br>
            <h6><a href="https://github.com/vlarie/EPIC_team5_Project3/tree/tKeypointUpdates">Project Repo</a></h6>
        </section>
    </div>
</body>

<!-- FOOTER AREA -->
{% include "footer.html" %}

<!-- SCRIPTS AREA -->
{% include "scripts.html" %}   

</html>